{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOXrjZaporz84OD2R+boODB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jh4cZGXHd2IW","executionInfo":{"status":"ok","timestamp":1721796593981,"user_tz":-480,"elapsed":7,"user":{"displayName":"Lin Johnson","userId":"07221548286512557189"}},"outputId":"ae5cfa81-a11e-45bc-9baf-6f7e3b6ea348"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1000, 50)\n","[[ 1.11040058  0.66562765  0.23665935 ...  0.00526888 -0.0256904\n","  -0.0476118 ]\n"," [ 0.66562765  1.19340881  0.61816087 ...  0.02506802 -0.01604705\n","   0.00840254]\n"," [ 0.23665935  0.61816087  1.15360688 ...  0.05120017  0.05253062\n","   0.01766275]\n"," ...\n"," [ 0.00526888  0.02506802  0.05120017 ...  1.31611301  0.718714\n","   0.31890449]\n"," [-0.0256904  -0.01604705  0.05253062 ...  0.718714    1.251185\n","   0.70383651]\n"," [-0.0476118   0.00840254  0.01766275 ...  0.31890449  0.70383651\n","   1.0694355 ]]\n"]}],"source":["import numpy as np\n","\n","dim =50    #矩阵维度\n","\n","# 设置高斯分布的参数\n","mean = np.zeros(dim)  # dim维的均值向量\n","cov = np.eye(dim)     # dim维的协方差矩阵（单位矩阵）\n","\n","# 生成num_samples个dim维的高斯分布样本\n","num_samples = 1000\n","data = np.random.multivariate_normal(mean, cov, num_samples)\n","\n","r=0.1\n","C = np.zeros([dim,dim])\n","for i in range(dim):\n","    for j in range(dim):\n","        C[i,j]=r**abs(i-j)\n","\n","data = data@(C**0.5)\n","\n","data_eval = np.random.multivariate_normal(mean, cov, num_samples)@(C**0.5)\n","print(data.shape)\n","\n","# 计算样本协方差矩阵\n","SCM = data_eval.T@data_eval/num_samples\n","print(SCM)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, n, m, d_model, nhead, num_encoder_layers):\n","        super(TransformerModel, self).__init__()\n","        self.embedding = nn.Linear(m, d_model)\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model, nhead), num_encoder_layers)\n","        self.fc = nn.Linear(d_model, m)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.transformer(x)\n","        x = x.mean(dim=0)\n","        w = self.fc(x)\n","        w = self.softmax(w)\n","        return w\n","\n","def compute_loss(x, w):\n","    r = torch.matmul(x, w)\n","    r_mean = r.mean()\n","    loss = ((r - r_mean) ** 2).mean()  # 方差\n","    return loss\n","\n","# 数据\n","n = 1000  # 天数\n","m = 50   # 股票数量\n","d_model = 64\n","nhead = 8\n","num_encoder_layers = 3\n","\n","x = torch.from_numpy(data).float()\n","\n","# 模型\n","model = TransformerModel(n, m, d_model, nhead, num_encoder_layers)\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# 训练\n","epochs = 200\n","k = 2  # 分割数量\n","s = int(n / k)  #  每份数量\n","train_ratio = 0.8\n","train_losses = []\n","test_losses = []\n","for epoch in range(epochs):\n","    total_train_loss = 0\n","    total_test_loss = 0\n","    for i in range(k):\n","        # 分割数据\n","        train_data = x[i*s:int(i*s+s*train_ratio)]\n","        test_data = x[int(i*s+s*train_ratio):i*s+s]\n","\n","        model.train()\n","        optimizer.zero_grad()\n","        w = model(train_data)\n","        train_loss = compute_loss(test_data, w)\n","        train_loss.backward()\n","\n","        # # 梯度裁剪\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","\n","        # 计算测试损失\n","        model.eval()\n","        with torch.no_grad():\n","            test_loss = compute_loss(test_data, w)\n","\n","        total_train_loss += train_loss.item()\n","        total_test_loss += test_loss.item()\n","\n","    avg_train_loss = total_train_loss / k\n","    avg_test_loss = total_test_loss / k\n","    train_losses.append(avg_train_loss)\n","    test_losses.append(avg_test_loss)\n","\n","    if epoch % 10 == 0:\n","        # print(f'Epoch {epoch}, Avg Train Loss: {avg_train_loss}, Avg Test Loss: {avg_test_loss}')\n","        print(f'Epoch {epoch}, Avg Loss: {avg_train_loss}')\n","\n","# 训练结果\n","w2 = model(x[:80]).detach().numpy()\n","print(\"权重:\", w2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Yb3fynBn6S9","executionInfo":{"status":"ok","timestamp":1721796639710,"user_tz":-480,"elapsed":41157,"user":{"displayName":"Lin Johnson","userId":"07221548286512557189"}},"outputId":"e8f1e516-c66c-412a-abc6-1b9a41d9229b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, Avg Loss: 0.07443331927061081\n","Epoch 10, Avg Loss: 0.06275120563805103\n","Epoch 20, Avg Loss: 0.06138725392520428\n","Epoch 30, Avg Loss: 0.06077142432332039\n","Epoch 40, Avg Loss: 0.060169585049152374\n","Epoch 50, Avg Loss: 0.05940084904432297\n","Epoch 60, Avg Loss: 0.058356428518891335\n","Epoch 70, Avg Loss: 0.0570683553814888\n","Epoch 80, Avg Loss: 0.05570976622402668\n","Epoch 90, Avg Loss: 0.05445376597344875\n","Epoch 100, Avg Loss: 0.0535771269351244\n","Epoch 110, Avg Loss: 0.0529395267367363\n","Epoch 120, Avg Loss: 0.05246035382151604\n","Epoch 130, Avg Loss: 0.052081283181905746\n","Epoch 140, Avg Loss: 0.05175739526748657\n","Epoch 150, Avg Loss: 0.05148409679532051\n","Epoch 160, Avg Loss: 0.05124539136886597\n","Epoch 170, Avg Loss: 0.051041822880506516\n","Epoch 180, Avg Loss: 0.050852783024311066\n","Epoch 190, Avg Loss: 0.05069912038743496\n","权重: [0.04081721 0.02163453 0.0073274  0.01136523 0.03702924 0.0150595\n"," 0.04635182 0.0007922  0.0193604  0.02416335 0.0035845  0.02400509\n"," 0.01640315 0.00216026 0.00529795 0.02045242 0.03157391 0.00243332\n"," 0.04474218 0.07993886 0.01761014 0.0070885  0.07688516 0.00396808\n"," 0.00305331 0.00641972 0.00262742 0.00367782 0.02071355 0.01498193\n"," 0.00291451 0.00481063 0.04275733 0.00241384 0.01478844 0.00258932\n"," 0.00258436 0.04142962 0.01010514 0.06976616 0.00138314 0.00606754\n"," 0.02681346 0.03568424 0.00726008 0.02628915 0.01242771 0.00480321\n"," 0.02989972 0.0436942 ]\n"]}]},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    w_nn = model(torch.from_numpy(data_eval).float())  # 计算新的权重\n","print(w_nn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyirbEuJzmWE","executionInfo":{"status":"ok","timestamp":1721796643534,"user_tz":-480,"elapsed":428,"user":{"displayName":"Lin Johnson","userId":"07221548286512557189"}},"outputId":"54838069-43b2-46a6-ccf1-a59f609f4832"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0471, 0.0153, 0.0081, 0.0131, 0.0334, 0.0117, 0.0479, 0.0007, 0.0131,\n","        0.0310, 0.0027, 0.0188, 0.0112, 0.0027, 0.0050, 0.0168, 0.0247, 0.0027,\n","        0.0574, 0.0794, 0.0180, 0.0093, 0.0878, 0.0062, 0.0028, 0.0104, 0.0022,\n","        0.0037, 0.0287, 0.0103, 0.0045, 0.0063, 0.0274, 0.0026, 0.0138, 0.0029,\n","        0.0024, 0.0376, 0.0091, 0.0772, 0.0011, 0.0058, 0.0157, 0.0476, 0.0064,\n","        0.0206, 0.0164, 0.0062, 0.0322, 0.0422])\n"]}]},{"cell_type":"code","source":["def risk_true(C):\n","  # 创建全 1 向量\n","  ones_vector = np.ones(50)\n","  return 1/np.dot(ones_vector.T, np.dot(np.linalg.inv(C), ones_vector))\n","\n","def risk_nn(w):\n","  return np.dot(w.T, np.dot(C, w))\n","\n","def risk_scm(SCM, C):\n","  ones_vector = np.ones(50)\n","\n","  SCM_inv = np.linalg.inv(SCM)\n","\n","  # 计算 Σ^(-1) 1\n","  inv_ones_vector = np.dot(SCM_inv, ones_vector)\n","\n","  # 计算 1^T Σ^(-1) 1\n","  scalar = np.dot(ones_vector.T, inv_ones_vector)\n","\n","  w_scm = inv_ones_vector / scalar\n","  return np.dot(w_scm.T, np.dot(C, w_scm))\n","\n","\n","print(\"risk_ture:\", risk_true(C))\n","print(\"risk_nn:\", risk_nn(w2))\n","print(\"risk_scm:\", risk_scm(SCM, C))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gULViq8vwyV","executionInfo":{"status":"ok","timestamp":1721796646217,"user_tz":-480,"elapsed":433,"user":{"displayName":"Lin Johnson","userId":"07221548286512557189"}},"outputId":"eec02c64-1a96-45df-d888-d8bff12bcacc"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["risk_ture: 0.0243362831858407\n","risk_nn: 0.042940504833107095\n","risk_scm: 0.03331865776308151\n"]}]}]}